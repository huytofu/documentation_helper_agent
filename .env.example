# Environment variables for the application

# Server configuration
PORT=8000
SERVER_TYPE=vercel  # Options: local, aws lambda, azure functions, google cloud functions, vercel

# LangGraph configuration
FLOW=real  # Options: real, test, simple
CHECKPOINTER_TYPE=vercel_kv  # Options: memory, vercel_kv, postgres

# Vector store configuration
VECTOR_STORE_TYPE=pinecone  # Options: chroma, pinecone

# Pinecone configuration (required if VECTOR_STORE_TYPE=pinecone)
PINECONE_API_KEY=
PINECONE_ENVIRONMENT=us-east1-aws
PINECONE_INDEX_NAME=documentation-helper-agent
PINECONE_DIMENSION=1024
PINECONE_INDEX_TYPE=dense
PINECONE_METRIC=cosine

# Vercel KV configuration (required if CHECKPOINTER_TYPE=vercel_kv)
KV_URL=
KV_REST_API_URL=
KV_REST_API_TOKEN=
KV_REST_API_READ_ONLY_TOKEN=

# PostgreSQL configuration (required if CHECKPOINTER_TYPE=postgres)
DATABASE_URL=

# Model Provider Selection (Default: Hugging Face Inference API for most models, RunPod for generator)
USE_OLLAMA=false
USE_HUGGINGFACE=false
USE_INFERENCE_CLIENT=false
USE_RUNPOD=true

# Model configuration
INFERENCE_PROVIDER=together  # Options: together, perplexity, anyscale, etc.
INFERENCE_API_KEY=
INFERENCE_EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
INFERENCE_GRADER_MODEL=deepseek-ai/DeepSeek-R1
INFERENCE_ROUTER_MODEL=deepseek-ai/DeepSeek-R1
INFERENCE_GENERATOR_MODEL=deepseek-ai/DeepSeek-Coder-V2

# Logging configuration
LOG_LEVEL=INFO

# RunPod Configuration
RUNPOD_API_KEY=your_runpod_api_key
RUNPOD_ENDPOINT_ID=your_runpod_endpoint_id
RUNPOD_MODEL_ID=deepseek-ai/deepseek-coder-v2-instruct
RUNPOD_MAX_TOKENS=2048
RUNPOD_TEMPERATURE=0.2
RUNPOD_TOP_P=0.9
RUNPOD_TOP_K=40
RUNPOD_PRESENCE_PENALTY=0.1
RUNPOD_FREQUENCY_PENALTY=0.1
RUNPOD_USE_VLLM=true

# vLLM Specific Configuration
RUNPOD_VLLM_MAX_BATCHED_TOKENS=4096
RUNPOD_VLLM_MAX_NUM_SEQS=256
RUNPOD_VLLM_MAX_PADDINGS=256
RUNPOD_VLLM_GPU_MEMORY_UTILIZATION=0.9
RUNPOD_VLLM_MAX_MODEL_LEN=2048
RUNPOD_VLLM_QUANTIZATION=awq
RUNPOD_VLLM_DTYPE=float16

# Hugging Face Configuration
HUGGINGFACE_API_KEY=your_huggingface_api_key

# Model Selection (Default Models)
HUGGINGFACE_EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
HUGGINGFACE_GRADER_MODEL=meta-llama/Llama-3-70b-chat-hf
HUGGINGFACE_ROUTER_MODEL=meta-llama/Llama-3-70b-chat-hf
HUGGINGFACE_GENERATOR_MODEL=deepseek-ai/deepseek-coder-v2-instruct

# Concurrency Settings (Optimized for Vercel)
PROVISIONED_CONCURRENCY=1
CONCURRENCY_LIMIT=5

# Environment
ENVIRONMENT=development

# API Security
API_KEY=your_api_key

# Frontend URL (for CORS)
FRONTEND_URL=http://localhost:3000

# Security Settings
MAX_REQUEST_SIZE=1048576  # 1MB
MAX_BATCH_SIZE=10
REQUEST_TIMEOUT=30.0
RATE_LIMIT_REQUESTS=60
RATE_LIMIT_WINDOW=60
MAX_CONCURRENT_REQUESTS=100
CIRCUIT_BREAKER_THRESHOLD=0.8
CIRCUIT_BREAKER_RESET=60

# Firebase Configuration
NEXT_PUBLIC_FIREBASE_API_KEY=your_firebase_api_key
NEXT_PUBLIC_FIREBASE_AUTH_DOMAIN=your_firebase_auth_domain
NEXT_PUBLIC_FIREBASE_PROJECT_ID=your_firebase_project_id
NEXT_PUBLIC_FIREBASE_STORAGE_BUCKET=your_firebase_storage_bucket
NEXT_PUBLIC_FIREBASE_MESSAGING_SENDER_ID=your_firebase_messaging_sender_id
NEXT_PUBLIC_FIREBASE_APP_ID=your_firebase_app_id

# Encryption Key
NEXT_PUBLIC_ENCRYPTION_KEY=your_encryption_key

# Add other environment variables as needed 