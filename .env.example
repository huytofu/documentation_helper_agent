# Environment variables for the application

# Server configuration
PORT=8000
SERVER_TYPE=vercel  # Options: local, aws lambda, azure functions, google cloud functions, vercel

# LangGraph configuration
FLOW=real  # Options: real, test, simple
CHECKPOINTER_TYPE=vercel_kv  # Options: memory, vercel_kv, postgres

# Vector store configuration
VECTOR_STORE_TYPE=pinecone  # Options: chroma, pinecone

# Pinecone configuration (required if VECTOR_STORE_TYPE=pinecone)
PINECONE_API_KEY=
PINECONE_ENVIRONMENT=us-east1-aws
PINECONE_INDEX_NAME=documentation-helper-agent
PINECONE_DIMENSION=1024
PINECONE_INDEX_TYPE=dense
PINECONE_METRIC=cosine

# Vercel KV configuration (required if CHECKPOINTER_TYPE=vercel_kv)
KV_URL=
KV_REST_API_URL=
KV_REST_API_TOKEN=
KV_REST_API_READ_ONLY_TOKEN=

# PostgreSQL configuration (required if CHECKPOINTER_TYPE=postgres)
DATABASE_URL=

# Model Provider Selection (Default: Hugging Face Inference API for most models, RunPod for generator)
USE_OLLAMA=false
USE_HUGGINGFACE=true
USE_INFERENCE_CLIENT=false
USE_RUNPOD=true  # Set to true to use RunPod serverless for generator model

# Model configuration
INFERENCE_PROVIDER=together  # Options: together, perplexity, anyscale, etc.
INFERENCE_API_KEY=
INFERENCE_EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
INFERENCE_GRADER_MODEL=deepseek-ai/DeepSeek-R1
INFERENCE_ROUTER_MODEL=deepseek-ai/DeepSeek-R1
INFERENCE_GENERATOR_MODEL=deepseek-ai/DeepSeek-Coder-V2

# Logging configuration
LOG_LEVEL=INFO

# RunPod Configuration (if USE_RUNPOD=true)
RUNPOD_API_KEY=your_runpod_api_key
RUNPOD_ENDPOINT_ID=your_endpoint_id
RUNPOD_MODEL_ID=deepseek-ai/deepseek-coder-v2-instruct
RUNPOD_MAX_TOKENS=2048
RUNPOD_TEMPERATURE=0.2
RUNPOD_TOP_P=0.9
RUNPOD_TOP_K=40
RUNPOD_PRESENCE_PENALTY=0.1
RUNPOD_FREQUENCY_PENALTY=0.1

# Hugging Face Configuration
HUGGINGFACE_API_KEY=your_huggingface_api_key

# Model Selection (Default Models)
HUGGINGFACE_EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
HUGGINGFACE_GRADER_MODEL=meta-llama/Llama-3-70b-chat-hf
HUGGINGFACE_ROUTER_MODEL=meta-llama/Llama-3-70b-chat-hf
HUGGINGFACE_GENERATOR_MODEL=deepseek-ai/deepseek-coder-v2-instruct

# Concurrency Settings (Optimized for Vercel)
PROVISIONED_CONCURRENCY=1
CONCURRENCY_LIMIT=5

# Environment
ENVIRONMENT=production

# API Security
API_KEY=your_api_key

# Frontend URL (for CORS)
FRONTEND_URL=http://localhost:3000

# Add other environment variables as needed 