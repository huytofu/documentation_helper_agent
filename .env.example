#Tavily and Langsmith Tracing
TAVILY_API_KEY=your_tavily_api_key
#Langsmith Tracing
LANGSMITH_TRACING=true
LANGSMITH_PROJECT=documentation_helper_agent
LANGSMITH_API_KEY=your_langsmith_api_key
LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# Model Provider Selection (Default: Hugging Face Inference API for most models, RunPod for generator)
USE_OLLAMA=false
# Ollama settings (if using Ollama)
OLLAMA_BASE_URL=http://localhost:11434

USE_HUGGINGFACE=true
USE_INFERENCE_CLIENT=false
USE_RUNPOD=false

# Server configuration
PORT=8000
SERVER_TYPE=vercel  # Options: local, aws lambda, vercel
VERCEL_URL=localhost:3000
# Concurrency Settings (Optimized for Vercel)
PROVISIONED_CONCURRENCY=1
CONCURRENCY_LIMIT=5
# LangGraph configuration
FLOW=real  # Options: real, test, simple
LOG_LEVEL=INFO
# LangGraph Checkpointer Configuration
CHECKPOINTER_TYPE=redis  # Options: memory, vercel_kv, postgres, redis
# Vercel KV configuration (required if CHECKPOINTER_TYPE=vercel_kv)
KV_URL=
KV_REST_API_URL=
KV_REST_API_TOKEN=
KV_REST_API_READ_ONLY_TOKEN=
# PostgreSQL configuration (required if CHECKPOINTER_TYPE=postgres)
DATABASE_URL=
#Redis configuration (required if CHECKPOINTER_TYPE=redis)
REDIS_URL=your_redis_url

# Vector store configuration
VECTOR_STORE_TYPE=pinecone  # Options: chroma, pinecone
# Pinecone configuration (required if VECTOR_STORE_TYPE=pinecone)
PINECONE_API_KEY=your_pinecone_api_key
PINECONE_INDEX_NAME=documentation-helper-agent
PINECONE_DIMENSION=1024
PINECONE_INDEX_TYPE=dense
PINECONE_METRIC=cosine

# Model configuration
INFERENCE_PROVIDER=together  # Options: together, perplexity, anyscale, etc.
INFERENCE_API_KEY=inference_provider_api_key
INFERENCE_EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
INFERENCE_ROUTER_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
INFERENCE_SENTIMENT_GRADER_MODEL=mistralai/Mistral-7B-Instruct-v0.3
INFERENCE_ANSWER_GRADER_MODEL=mistralai/Mistral-7B-Instruct-v0.3
INFERENCE_RETRIEVAL_GRADER_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
INFERENCE_HALLUCINATE_GRADER_MODEL=Qwen/Qwen2.5-14B-Instruct
INFERENCE_SUMMARIZER_MODEL=Qwen/Qwen2.5-14B-Instruct
INFERENCE_GENERATOR_MODEL=deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct

# RunPod Configuration
RUNPOD_API_KEY=your_runpod_api_key
RUNPOD_ENDPOINT_ID=your_runpod_endpoint_id
RUNPOD_MODEL_ID=deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct
RUNPOD_MAX_TOKENS=2048
RUNPOD_TEMPERATURE=0.2
RUNPOD_TOP_P=0.9
RUNPOD_TOP_K=40
RUNPOD_PRESENCE_PENALTY=0.1
RUNPOD_FREQUENCY_PENALTY=0.1
RUNPOD_USE_VLLM=true
RUNPOD_TRUST_REMOTE_CODE=true
# vLLM Specific Configuration
RUNPOD_VLLM_MAX_BATCHED_TOKENS=4096
RUNPOD_VLLM_MAX_NUM_SEQS=256
RUNPOD_VLLM_MAX_PADDINGS=256
RUNPOD_VLLM_GPU_MEMORY_UTILIZATION=0.9
RUNPOD_VLLM_MAX_MODEL_LEN=2048
RUNPOD_VLLM_QUANTIZATION=awq
RUNPOD_VLLM_DTYPE=float16

# Hugging Face Configuration
HUGGINGFACE_API_KEY=your_huggingface_api_key
HUGGINGFACE_EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
HUGGINGFACE_ROUTER_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
HUGGINGFACE_SENTIMENT_GRADER_MODEL=mistralai/Mistral-7B-Instruct-v0.3
HUGGINGFACE_ANSWER_GRADER_MODEL=mistralai/Mistral-7B-Instruct-v0.3
HUGGINGFACE_RETRIEVAL_GRADER_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
HUGGINGFACE_HALLUCINATE_GRADER_MODEL=Qwen/Qwen2.5-14B-Instruct
HUGGINGFACE_SUMMARIZER_MODEL=Qwen/Qwen2.5-14B-Instruct
HUGGINGFACE_GENERATOR_MODEL=deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct
HUGGINGFACE_MAX_TOKENS=2048

# Security Settings
MAX_REQUEST_SIZE=1048576  # 1MB
MAX_BATCH_SIZE=10
REQUEST_TIMEOUT=30.0
RATE_LIMIT_REQUESTS=60
RATE_LIMIT_WINDOW=60
MAX_CONCURRENT_REQUESTS=100
CIRCUIT_BREAKER_THRESHOLD=0.8
CIRCUIT_BREAKER_RESET=60

# Firebase Configuration
NEXT_PUBLIC_FIREBASE_API_KEY=your_firebase_api_key
NEXT_PUBLIC_FIREBASE_AUTH_DOMAIN=your_firebase_auth_domain
NEXT_PUBLIC_FIREBASE_PROJECT_ID=your_firebase_project_id
NEXT_PUBLIC_FIREBASE_STORAGE_BUCKET=your_firebase_storage_bucket
NEXT_PUBLIC_FIREBASE_MESSAGING_SENDER_ID=your_firebase_messaging_sender_id
NEXT_PUBLIC_FIREBASE_APP_ID=your_firebase_app_id
FIREBASE_SERVICE_ACCOUNT=your_firebase_service_account
FIREBASE_CLIENT_EMAIL=your_client_email_here
FIREBASE_PRIVATE_KEY=your_private_key_here
# Frontend configuration
NEXT_PUBLIC_API_URL=/api
DOCUMENTATION_HELPER_API_KEY=your_api_key
ENCRYPTION_KEY=your_encryption_key
FRONTEND_URL=http://localhost:3000
BACKEND_URL=http://localhost:8000