# Environment variables for the application

# Server configuration
PORT=8000
SERVER_TYPE=local  # Options: local, aws lambda, azure functions, google cloud functions, vercel

# LangGraph configuration
FLOW=real  # Options: real, test, simple
CHECKPOINTER_TYPE=memory  # Options: memory, vercel_kv, postgres

# Vector store configuration
VECTOR_STORE_TYPE=chroma  # Options: chroma, pinecone

# Pinecone configuration (required if VECTOR_STORE_TYPE=pinecone)
PINECONE_API_KEY=
PINECONE_ENVIRONMENT=
PINECONE_INDEX_NAME=

# Vercel KV configuration (required if CHECKPOINTER_TYPE=vercel_kv)
KV_URL=
KV_REST_API_URL=
KV_REST_API_TOKEN=
KV_REST_API_READ_ONLY_TOKEN=

# PostgreSQL configuration (required if CHECKPOINTER_TYPE=postgres)
DATABASE_URL=

# Model configuration
USE_INFERENCE_CLIENT=false  # Set to true to use InferenceClient with third-party providers
INFERENCE_PROVIDER=together  # Options: together, perplexity, anyscale, etc.
INFERENCE_API_KEY=
INFERENCE_EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
INFERENCE_GRADER_MODEL=deepseek-ai/DeepSeek-R1
INFERENCE_ROUTER_MODEL=deepseek-ai/DeepSeek-R1
INFERENCE_GENERATOR_MODEL=deepseek-ai/DeepSeek-Coder-V2

# Logging configuration
LOG_LEVEL=INFO

# Add other environment variables as needed 